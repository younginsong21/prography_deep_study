{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nName classification task using RNN model \\n지금까지의 char-char prediction은 many to many의 구조\\n이 경우는 many(name) to one(class)의 구조\\n\\nchar => ascii code as index => embedding cell => RNN => softmax output\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Name classification task using RNN model \n",
    "지금까지의 char-char prediction은 many to many의 구조\n",
    "이 경우는 many(name) to one(class)의 구조\n",
    "\n",
    "char => ascii code as index => embedding cell => RNN => softmax output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리\n",
    "import time\n",
    "import math\n",
    "import csv\n",
    "import gzip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name Dataset 클래스 정의\n",
    "class NameDataset(Dataset):\n",
    "    # 데이터 파일 가져와서 추출\n",
    "    def __init__(self, is_train_set=False):\n",
    "        if is_train_set: # training set인 경우\n",
    "            filename = './data/names_train.csv.gz'\n",
    "        else:            # test set인 경우\n",
    "            filename = './data/names_test.csv.gz'\n",
    "        with gzip.open(filename, \"rt\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            rows = list(reader)\n",
    "        \n",
    "        self.names = [row[0] for row in rows]\n",
    "        self.countries = [row[1] for row in rows]\n",
    "        self.len = len(self.countries)\n",
    "        \n",
    "        # 데이터셋에 포함된 나라를 중복없이 담은 리스트\n",
    "        self.country_list = list(sorted(set(self.countries)))\n",
    "    \n",
    "    # row index가 주어졌을 때 해당 row의 item 가져오는 함수\n",
    "    def __getitem__(self, idx):\n",
    "        return self.names[idx], self.countries[idx]\n",
    "    \n",
    "    # 전체 데이터셋의 길이\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    # country_list 가져오는 함수\n",
    "    def get_countries(self):\n",
    "        return self.country_list\n",
    "    \n",
    "    # country_list에서 특정 idx의 country만 가져오는 함수\n",
    "    # idx로 country에 접근\n",
    "    def idx2country(self, idx):\n",
    "        return self.country_list[idx]\n",
    "    \n",
    "    # country_list에서 특정 country의 idx만 가져오는 함수\n",
    "    # country로 idx에 접근\n",
    "    def country2idx(self, country):\n",
    "        return self.country_list.index(country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters & DataLoaders\n",
    "emb_size = 100\n",
    "hidden_size = 100\n",
    "num_chars = 128 # ASCII\n",
    "num_classes = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN classifier 클래스 정의\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, emb_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.emb_size = emb_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.input_size, self.emb_size)\n",
    "        self.gru = nn.GRU(self.emb_size, self.hidden_size, self.n_layers)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # input's shape: (batch_size, sequence_length)\n",
    "        batch_size = input.size(0)\n",
    "        \n",
    "        # transpose 취함. \n",
    "        # input's shape: (sequence_length, batch_size)\n",
    "        input = input.t()\n",
    "        \n",
    "        # Embedding forwarding\n",
    "        # embedding output's shape: (sequence_length, batch_size, emb_size)\n",
    "        emb_out = self.embedding(input)\n",
    "        \n",
    "        # hidden 초기화\n",
    "        hidden = self._init_hidden(batch_size)\n",
    "        \n",
    "        # RNN forwarding\n",
    "        \n",
    "        rnn_out, hidden = self.gru(emb_out, hidden)\n",
    "        print(\"rnn_out.shape:\", rnn_out.shape)\n",
    "        print(\"hidden.shape:\", hidden.shape)\n",
    "        \n",
    "        # FC forwarding\n",
    "        # 마지막 cell의 hidden == 마지막 cell의 output이라서 FC input으로 hidden 쓴다.\n",
    "        fc_out = self.fc(hidden)\n",
    "        print(fc_out.shape)\n",
    "        \n",
    "        return fc_out\n",
    "        \n",
    "    def _init_hidden(self, batch_size):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_size)\n",
    "        return Variable(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string => ASCII array로 변환 함수\n",
    "def str2ascii_arr(string):\n",
    "    arr = [ord(c) for c in string]\n",
    "    return arr, len(arr)\n",
    "\n",
    "# 여러개의 input string을 처리할 때 sequence_length를 맞추기 위한 zero padding\n",
    "# 함수의 인자 input_seq: padding 처리하고 싶은 input 문자열들 (여러개)\n",
    "#             seq_lengths: 각 문자열들의 길이 (여러개)\n",
    "def zero_padding_sequences(vectorized_seqs, seq_lengths):\n",
    "    seq_tensor = torch.zeros((len(vectorized_seqs), seq_lengths.max())).long()\n",
    "    for idx, (seq, seq_len) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        seq_tensor[idx, :seq_len] = torch.LongTensor(seq) # 앞에서부터 원래 seq 길이까지는 원래 seq들로 채우고 뒤에는 초기화한대로 0이 됨\n",
    "        \n",
    "    return seq_tensor\n",
    "\n",
    "def make_variables(names):\n",
    "    sequence_and_length = [str2ascii_arr(name) for name in names]\n",
    "    vectorized_seqs = [sl[0] for sl in sequence_and_length]  # name ascii code만 저장된 리스트\n",
    "    \n",
    "    seq_lengths = torch.LongTensor([sl[1] for sl in sequence_and_length]) # name 길이만 저장된 리스트\n",
    "    return zero_padding_sequences(vectorized_seqs, seq_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<<< without zero padding >>>\n",
      "input name: adylov\n",
      "rnn_out.shape: torch.Size([6, 1, 100])\n",
      "hidden.shape: torch.Size([1, 1, 100])\n",
      "torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 6]) out torch.Size([1, 1, 18])\n",
      "\n",
      "\n",
      "input name: solan\n",
      "rnn_out.shape: torch.Size([5, 1, 100])\n",
      "hidden.shape: torch.Size([1, 1, 100])\n",
      "torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 5]) out torch.Size([1, 1, 18])\n",
      "\n",
      "\n",
      "input name: hard\n",
      "rnn_out.shape: torch.Size([4, 1, 100])\n",
      "hidden.shape: torch.Size([1, 1, 100])\n",
      "torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 4]) out torch.Size([1, 1, 18])\n",
      "\n",
      "\n",
      "input name: san\n",
      "rnn_out.shape: torch.Size([3, 1, 100])\n",
      "hidden.shape: torch.Size([1, 1, 100])\n",
      "torch.Size([1, 1, 18])\n",
      "in torch.Size([1, 3]) out torch.Size([1, 1, 18])\n",
      "\n",
      "\n",
      "\n",
      "<<< with zero padding>>>\n",
      "rnn_out.shape: torch.Size([6, 4, 100])\n",
      "hidden.shape: torch.Size([1, 4, 100])\n",
      "torch.Size([1, 4, 18])\n",
      "batch in torch.Size([4, 6]) batch out torch.Size([1, 4, 18])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    names = ['adylov', 'solan', 'hard', 'san']\n",
    "    classifier = RNNClassifier(num_chars, emb_size, hidden_size, num_classes)\n",
    "    \n",
    "    print(\"<<< without zero padding >>>\")\n",
    "    for name in names:\n",
    "        print(\"input name:\", name)\n",
    "        arr, _ = str2ascii_arr(name)\n",
    "        inp = Variable(torch.LongTensor([arr]))\n",
    "        out = classifier(inp)\n",
    "        print(\"in\", inp.size(), \"out\", out.size())\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"\\n<<< with zero padding>>>\")\n",
    "    inputs = make_variables(names)\n",
    "    out = classifier(inputs)\n",
    "    print(\"batch in\", inputs.size(), \"batch out\", out.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
