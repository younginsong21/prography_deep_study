{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1cd677ce950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hihell => ihello 예제를 응용해서 조금 더 긴 문장을 prediction하는 예제\n",
    "# 필요한 라이브러리\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data\n",
    "sentence = \" hello pytorch! let's study deep learning!\"\n",
    "char_set = set(sentence)\n",
    "idx2char = list(char_set) # index -> char 접근\n",
    "char2idx = {c:i for i, c in enumerate(idx2char)} # char -> index 접근\n",
    "\n",
    "sen2list = list(sentence)\n",
    "\n",
    "# Define X data\n",
    "x_data = sen2list[:-1]\n",
    "x_data = [char2idx[c] for c in x_data]\n",
    "x_data = [x_data]\n",
    "\n",
    "# Define Y data\n",
    "y_data = sen2list[1:]\n",
    "y_data = [char2idx[c] for c in y_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 15,   7,   0,  12,  12,   3,  15,  13,  10,   9,   3,   1,\n",
      "           8,   7,   2,  15,  12,   0,   9,  11,   4,  15,   4,   9,\n",
      "           5,   6,  10,  15,   6,   0,   0,  13,  15,  12,   0,  14,\n",
      "           1,  17,  16,  17,  18]])\n",
      "tensor([  7,   0,  12,  12,   3,  15,  13,  10,   9,   3,   1,   8,\n",
      "          7,   2,  15,  12,   0,   9,  11,   4,  15,   4,   9,   5,\n",
      "          6,  10,  15,   6,   0,   0,  13,  15,  12,   0,  14,   1,\n",
      "         17,  16,  17,  18,   2])\n"
     ]
    }
   ],
   "source": [
    "# Array -> Tensor 변환\n",
    "inputs = Variable(torch.LongTensor(x_data))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyper parameters\n",
    "num_classes = len(char2idx)\n",
    "input_size = len(char2idx)\n",
    "embedding_size = 38  # embedding size (19->38 encoding)\n",
    "hidden_size = len(char2idx)  # 바로 one hot으로 예측하기 위해 19로 설정\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = len(y_data)\n",
    "num_layers = 1  # one-layer rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 모델 정의\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, embedding_size, hidden_size, num_layers):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # assign\n",
    "        self.num_classes = num_classes\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # embedding cell 정의\n",
    "        # input_size => embedding_size\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        \n",
    "        # RNN cell 정의\n",
    "        # embedding size => hidden_size\n",
    "        self.rnn = nn.RNN(input_size=self.embedding_size,\n",
    "                          hidden_size=self.hidden_size, batch_first=True)\n",
    "        \n",
    "        # FC layer 정의\n",
    "        # hidden_size => num_classes\n",
    "        self.fc = nn.Linear(self.hidden_size, self.num_classes)\n",
    "    \n",
    "    \"\"\"\n",
    "    각 단계에서의 input shape와 output shape::\n",
    "    initial input X            (batch_size, seq_length)\n",
    "          ||\n",
    "    [embedding cell]\n",
    "          ||\n",
    "    embedding output           (batch_size, seq_length, emb_size)\n",
    "          ||\n",
    "    reshaped for RNN           (batch_size, seq_length, -1)\n",
    "          ||\n",
    "      [RNN cells]\n",
    "          ||\n",
    "       RNN output              (batch_size, seq_length, hidden_size)\n",
    "          ||\n",
    "    reshaped for FC            (seq_length, num_classes)\n",
    "          ||\n",
    "       [FC layer]\n",
    "          ||\n",
    "       FC output               (seq_length, num_classes)\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        # hidden layer 초기화\n",
    "        # (layer 개수, batch size, hidden size)\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # 최초의 input인 X의 shape: (batch size, seq length)\n",
    "        \n",
    "        # embedding forwarding\n",
    "        # 이 때 emb_out의 shape: (batch size, seq length, emb_size)\n",
    "        emb_out = self.embedding(x)\n",
    "        \n",
    "        # Reshape Embedding output\n",
    "        # (batch size, sequence length, embedding size) => (batch size, seq length, -1)\n",
    "        emb_out = emb_out.view(batch_size, sequence_length, -1)\n",
    "\n",
    "        # RNN forwarding\n",
    "        # 이 때 rnn_out의 shape: (batch size, seq length, hidden size)\n",
    "        rnn_out, _ = self.rnn(emb_out, h_0)\n",
    "        \n",
    "        # Reshape RNN output\n",
    "        # (batch size, seq length, hidden size) => (seq length, num classes)\n",
    "        rnn_out = rnn_out.view(-1, self.num_classes)\n",
    "        \n",
    "        # FC layer forwarding\n",
    "        # 이 때 fc_out의 shape: (seq length, num_classes)\n",
    "        fc_out = self.fc(rnn_out)\n",
    "        \n",
    "        return fc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(19, 38)\n",
      "  (rnn): RNN(38, 19, batch_first=True)\n",
      "  (fc): Linear(in_features=19, out_features=19, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델 정의\n",
    "model = Model(num_classes, input_size, embedding_size, hidden_size, num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss, optimizer 정의\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss: 3.026\n",
      "Predicted string:  !o''''ya'!d'p '!'d'd ! 'aa!ydd'a!pds!'p'd\n",
      "epoch: 2, loss: 2.184\n",
      "Predicted string:  l!pee l  lrn ! le l! l!ld el pp le! nini!\n",
      "epoch: 3, loss: 1.480\n",
      "Predicted string:  lelee letornh! lea'e ltudeeoelp lea nini!\n",
      "epoch: 4, loss: 0.991\n",
      "Predicted string:  lelle letorch! lea's ltudytoelp learcing!\n",
      "epoch: 5, loss: 0.694\n",
      "Predicted string:  lellerpyturch! pea's st'dytdyep learcing!\n",
      "epoch: 6, loss: 0.462\n",
      "Predicted string:  lello lytorch! lea's study deep learning!\n",
      "epoch: 7, loss: 0.309\n",
      "Predicted string:  hello ly orch! lea's study deep learning!\n",
      "epoch: 8, loss: 0.223\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 9, loss: 0.150\n",
      "Predicted string:  hello pytorch! lea's study deep letrning!\n",
      "epoch: 10, loss: 0.114\n",
      "Predicted string:  hello pytorch! lea's study deep learning!\n",
      "epoch: 11, loss: 0.088\n",
      "Predicted string:  hello pytorch! lea's study deep learning!\n",
      "epoch: 12, loss: 0.071\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 13, loss: 0.060\n",
      "Predicted string:  hello pytorch! let's study deep letrning!\n",
      "epoch: 14, loss: 0.053\n",
      "Predicted string:  hello pytorch! let's study deep letrning!\n",
      "epoch: 15, loss: 0.047\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 16, loss: 0.040\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 17, loss: 0.030\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 18, loss: 0.023\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 19, loss: 0.017\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 20, loss: 0.013\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 21, loss: 0.009\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 22, loss: 0.007\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 23, loss: 0.005\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 24, loss: 0.004\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 25, loss: 0.004\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 26, loss: 0.003\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 27, loss: 0.003\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 28, loss: 0.002\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 29, loss: 0.002\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 30, loss: 0.002\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 31, loss: 0.002\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 32, loss: 0.002\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 33, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 34, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 35, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 36, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 37, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 38, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 39, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 40, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 41, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 42, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 43, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 44, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 45, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 46, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 47, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 48, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 49, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 50, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 51, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 52, loss: 0.001\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 53, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 54, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 55, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 56, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 57, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 58, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 59, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 60, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 61, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 62, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 63, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 64, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 65, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 66, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 67, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 68, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 69, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 70, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 71, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 72, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 73, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 74, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 75, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 76, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 77, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 78, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 79, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 80, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 81, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 82, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 83, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 84, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 85, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 86, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 87, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 88, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 89, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 90, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 91, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 92, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 93, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 94, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 95, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 96, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 97, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 98, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 99, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "epoch: 100, loss: 0.000\n",
      "Predicted string:  hello pytorch! let's study deep learning!\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습시키기\n",
    "for epoch in range(100):\n",
    "    # gradient -> zero 과정\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forwarding\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # loss 계산, backwarding 과정\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # class중 가장 높은 score 가진 index 뽑아내기\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs: tensor([[ -2.4168, -10.7533,   0.0159,   6.2247,   2.1006,   4.0881,\n",
      "           3.7938,  15.4936,  -5.2138,  -2.8205,  -6.3924,  -1.6705,\n",
      "           6.9083,   0.8580,  -5.1631,   0.4294,  -6.3680,   0.2369,\n",
      "          -8.6501],\n",
      "        [ 18.7029,  -0.8193,   8.2123,  -4.1436,   1.6646, -15.0959,\n",
      "          -1.0625,   0.0813,  -6.2607,   3.0577,   0.6252, -12.0387,\n",
      "           1.9686,  -3.1675,   4.1876,  -1.7998,  -6.7547,  -2.2124,\n",
      "          -0.7426],\n",
      "        [ -0.3456,   0.3015,  -1.4942,   0.6029,  -5.9279,  -0.1012,\n",
      "          -7.5759,  -2.8007,  -5.4827,   0.0809,  -9.4858,   4.0592,\n",
      "          15.1753,   5.8644,   3.4132,   3.4207,   3.2415,  -2.9595,\n",
      "           0.6885],\n",
      "        [  5.8381,  -6.7645,   0.8918,   0.1515,   5.4950,  -5.9333,\n",
      "          -3.0137,   4.0235,  -3.4755,   0.8498, -11.1970,  -0.8730,\n",
      "          15.7754,   2.9103,  -5.3685,  -5.9940,  -1.4441,  -0.8867,\n",
      "          -0.7581],\n",
      "        [  7.3001, -13.0816,   0.2215,  16.1081,  -4.3885,  -0.8059,\n",
      "           2.0440,   4.0401,  -5.1928,   6.7378,   3.7369,  -7.7311,\n",
      "          -0.0866, -13.1405,  -4.1605,   2.4720,   2.8950,  -8.1457,\n",
      "          -3.3292],\n",
      "        [ -3.9922,   7.4097,  -4.2258,  -1.8172, -10.3505,   3.5030,\n",
      "          -6.7357,  -2.7050,  -4.7861,   1.7282,   2.9844,   0.6949,\n",
      "           2.6048,  -2.6860,   0.6184,  16.8675,  -7.0680,  -0.7008,\n",
      "          -6.1266],\n",
      "        [ -2.6571,   0.9615,  -7.7708,  -2.9741,   8.2242,  -2.2448,\n",
      "           4.5249,   3.6784,   3.6236, -12.5249,  -5.4486,   6.4645,\n",
      "           8.6291,  16.7171,  -4.7303,  -4.2422,  -0.2680,  -0.9725,\n",
      "          -2.0420],\n",
      "        [  5.6097,  -5.0593,  -8.0651,   2.1107,  -8.5814,  -3.7383,\n",
      "           2.1644,   1.0358,  -1.4419,   4.3793,  16.0180, -10.7092,\n",
      "           0.9077,  -6.2543,   1.4986,   8.2999,  -7.1653,  -2.7556,\n",
      "          -7.0917],\n",
      "        [  1.0278,  -3.3235,  -5.5263,   2.6748,  -6.5320,   1.1288,\n",
      "          -3.8295,  -0.3439,   1.1108,  15.6372,   1.6920,  -0.2884,\n",
      "          -3.2138,  -6.7788,  -7.4096,   6.7381,   1.2166,  -2.1914,\n",
      "           1.3887],\n",
      "        [  4.7532,  -3.2180,  -4.4810,  15.0287,   0.2075,   3.4704,\n",
      "           3.7056,   0.3808,  -0.6526,  -3.6122,   3.0825,  -1.6884,\n",
      "          -7.7573,  -8.6889,  -2.1439,  -1.4782,  -1.4558,  -1.5931,\n",
      "          -3.0102],\n",
      "        [ -2.3529,  17.3323,  -4.0715,  -8.1489,  -1.4136,  -0.5948,\n",
      "          -6.7691,  -9.8983,   4.0232,  -0.5044,  -0.8679,   6.0441,\n",
      "          -3.5274,   4.5482,   1.7396,   6.9743,  -5.2917,   2.4016,\n",
      "           0.9186],\n",
      "        [ -0.5081,   2.2148, -11.1506,   2.4316,   0.3412,   0.9322,\n",
      "           0.2489,   0.5948,  17.2355,  -2.4344,  -2.3758,   1.9007,\n",
      "          -0.7178,   5.6400, -10.4179,  -0.7694,  -3.3365,   7.5559,\n",
      "           1.5598],\n",
      "        [  1.8694,  -6.3969,  -4.5427,   3.7567,   3.1172,  -3.6565,\n",
      "          -1.0653,  13.4551,  -2.3875,  -0.5685,   1.4947,  -4.9158,\n",
      "           0.4122,   0.8725,   2.4814,  -0.6445,  -0.2687,  -0.3451,\n",
      "          -5.1842],\n",
      "        [  4.8524,  -3.0595,  14.8443,  -6.2870,   3.6292,  -7.3111,\n",
      "           4.6472,   4.2934,  -9.5975,   1.8936,   1.3650,  -8.2894,\n",
      "           1.8534,  -4.6869,  -0.0239,   1.7035,  -5.3972,  -3.4457,\n",
      "          -0.1300],\n",
      "        [  4.9156,   5.2159,   5.4006,  -0.8711,  -9.3847,   0.9837,\n",
      "          -1.8449,  -0.7526,  -6.0341,   5.5436,   3.3940,  -7.2777,\n",
      "          -0.6834, -11.5834,  -0.0699,  14.8653, -14.8815,  -3.3103,\n",
      "          -6.1105],\n",
      "        [  2.0821,  -6.8673,  -0.8669,   0.9579,  -1.5737,   0.0979,\n",
      "           3.8876,   6.2321,  -5.3171,  -6.3147,  -3.1177,  -0.0124,\n",
      "          14.2401,   5.8074,  -4.2756,   1.8252,  -4.0563,  -6.4418,\n",
      "          -3.4306],\n",
      "        [ 19.2793,  -7.2492,   0.4508,   9.9529,  -1.9657,  -8.7605,\n",
      "           2.5033,  -1.3027,   0.9524,   3.5998,   2.4098, -12.2759,\n",
      "           0.8425,  -9.8432,  -4.7500,  -2.1099,  -6.6154,  -2.6888,\n",
      "          -2.7056],\n",
      "        [  2.1870,  -1.3755,   0.9502,   4.2300,  -8.1775,   1.5806,\n",
      "          -7.6075,  -6.1334, -11.1689,  14.6396,   1.7761,  -3.8346,\n",
      "          -1.6516, -10.3061,   6.6219,   6.2242,   1.1054,  -2.8863,\n",
      "          -4.8151],\n",
      "        [-14.0933,   6.9857,  -6.0430,   1.5873,  -0.0773,  10.4115,\n",
      "          -8.5084,  -2.1777,   2.4380,  -0.5738, -12.4098,  18.6319,\n",
      "           4.9941,   9.6464,  -3.9886,   2.8999,   5.3965,  -1.5211,\n",
      "           0.4208],\n",
      "        [  5.4786,   0.9568,   3.1742,  -8.4459,  14.8740, -13.0916,\n",
      "           6.3195,  -0.0953,   2.8167,  -2.9571,   1.3279,  -5.9959,\n",
      "          -2.1500,   5.7285,  -1.8002,  -9.0694,  -0.4424,   2.6343,\n",
      "           1.4608],\n",
      "        [ -5.2580,   2.6374,  -7.4884,   3.4507, -14.0191,   2.7406,\n",
      "          -5.0667,  -2.2772,   2.1421,   3.7541,   1.0778,   2.8781,\n",
      "           6.3588,  -0.7176,  -3.3661,  16.9075,   1.2152,  -3.4974,\n",
      "           0.0774],\n",
      "        [ -4.3559,  -3.5059,  -2.4777,  -0.3445,  17.2346,  -2.5732,\n",
      "           6.4577,   8.4159,  -0.7058, -11.6524,  -4.8797,   1.7186,\n",
      "           4.0659,   9.1946,  -2.1096, -11.5730,  -4.1497,   5.2237,\n",
      "          -7.7736],\n",
      "        [  4.0637,   0.3975,  -1.9953,   0.7736,  -8.2017,   0.5795,\n",
      "         -10.6787,  -0.5081,  -3.1617,  18.5902,  -5.5141,  -1.4200,\n",
      "           0.5382,  -3.5946,  -0.0317,   7.1148,  -2.2177,  -3.9410,\n",
      "          -4.2522],\n",
      "        [-12.6375,  -1.0600,  -1.3941,   5.6943,  -4.3237,  14.4993,\n",
      "           0.0045,   0.1942,  -3.1852,   1.8598,  -3.0436,   6.1909,\n",
      "           1.7322,  -3.2606,   0.1578,   3.1713,  -2.6392,   1.8289,\n",
      "          -4.6174],\n",
      "        [ -0.3345,  -5.4215,   2.6017,   1.4557,   8.4608,  -0.3252,\n",
      "          16.6589,   5.0276,   3.4541,  -5.7076,   1.3577,  -8.4743,\n",
      "          -2.9565,   2.9542,  -2.0836,  -7.4823,  -6.4725,   2.4884,\n",
      "          -3.3169],\n",
      "        [  7.8667,  -5.9482,  -0.8952,   5.2219,  -0.9751,  -5.8994,\n",
      "           7.9329,   5.0490,  -3.5748,  -0.9785,  16.1582, -12.2008,\n",
      "          -7.3525,  -9.8946,  -2.9517,   6.6513,  -8.2178,  -3.4741,\n",
      "          -7.8939],\n",
      "        [  5.8391,   3.1511,  -1.2107,   2.9483, -10.5760,  -0.9756,\n",
      "           0.3250,  -3.4569,  -2.4070,   8.0858,   6.0342,  -3.2947,\n",
      "          -5.1595,  -9.5691,  -6.5030,  15.8215,  -2.6607,  -9.9429,\n",
      "           2.0455],\n",
      "        [  6.1883,  -7.9679,   2.0184,   5.7278,   4.0218,  -2.9776,\n",
      "          15.1614,   5.2857,  -5.5911,  -8.7520,   4.7532, -11.6365,\n",
      "           1.4034,  -3.0936,  -0.2259,  -1.1621, -12.0957,   1.0881,\n",
      "         -10.0139],\n",
      "        [ 17.6608,  -1.7444,  -0.8147,   4.8698,  -3.1545,  -7.7659,\n",
      "           2.1510,   3.7251,  -0.1131,   0.8991,   7.3197, -10.9440,\n",
      "          -4.6939,  -9.7318,  -7.3072,   6.7817, -12.6475,  -4.1195,\n",
      "          -4.8469],\n",
      "        [ 16.3581,   0.2368,  -4.5217,   5.8155,  -7.6661,  -6.9687,\n",
      "          -5.1339,  -9.5486,  -4.3304,   4.8644,   5.8113,  -3.9643,\n",
      "           1.5317,  -3.1512,   4.1922,   2.9329,   1.3312, -10.6722,\n",
      "          -0.8991],\n",
      "        [ -2.4416,   0.9901,  -7.2768,  -6.2972,   3.6612,  -4.0372,\n",
      "          -7.6938,   0.5377,  -1.5281,   4.6870,  -8.2228,   5.2663,\n",
      "           7.0605,  15.6142,   5.0457,  -4.0466,   2.8156,   3.2495,\n",
      "          -4.7846],\n",
      "        [ -4.0910,   0.3593,  -4.5848,  -4.1389,  -9.8550,  -1.4882,\n",
      "          -5.6960,   2.2717,  -2.4548,   5.3066,   8.5170,  -3.7864,\n",
      "           5.7799,  -3.9106,  -1.6159,  15.8085,  -4.3393,  -0.1696,\n",
      "          -4.6365],\n",
      "        [ -2.7008,  -1.0563,   0.6345,  -3.0228,   3.4232,  -1.5126,\n",
      "           4.3511,   1.0420,  -7.2446,  -9.5616,  -3.0535,   0.4325,\n",
      "          14.5645,   5.0611,  -1.8955,   2.3216,  -5.6610,  -2.1541,\n",
      "          -4.9403],\n",
      "        [ 17.1816,  -3.0152,   2.9844,   4.0522,  -3.9909,  -9.3506,\n",
      "          -2.2335,  -7.2100,  -1.7816,   3.5639,  -0.3416,  -6.3236,\n",
      "           8.0923,  -6.1354,  -4.3362,  -0.1295,  -1.5342,  -7.6737,\n",
      "           2.9203],\n",
      "        [  4.2036,  -0.6632,  -0.2357,   0.2241,  -2.4972,  -3.2474,\n",
      "          -4.6606,  -4.6250, -11.5269,   6.0112,  -0.7596,  -3.6658,\n",
      "           4.5391,  -0.1382,  14.2060,  -2.4863,  -0.7261,  -0.7244,\n",
      "          -4.6548],\n",
      "        [ -3.1928,  17.0817,  -2.5716,  -9.6199,  -4.5644,  -0.5083,\n",
      "         -13.3055, -12.4313,   3.2508,   6.0682,  -4.5578,   6.9132,\n",
      "           0.5217,   2.1693,   1.2225,   8.4312,   0.1820,   1.1881,\n",
      "           5.0997],\n",
      "        [ -6.8745,   2.4280,  -4.3703,  -4.7897,   8.4066,  -0.2002,\n",
      "           0.7531,   2.5867,   7.9141,  -0.9694,  -8.4284,   0.2044,\n",
      "          -0.6095,   7.1785,  -1.4223,  -7.4434,  -4.6155,  16.9147,\n",
      "          -1.3643],\n",
      "        [ -1.8219,  -5.1889,  -2.6819,   4.9384,   1.2757,  -0.5274,\n",
      "          -1.2084,  -2.7528,  -3.1555,   3.8927,  -4.0767,   3.3244,\n",
      "           0.8029,   3.0819,   0.7229,  -6.2818,  17.0620,  -5.3565,\n",
      "           8.5023],\n",
      "        [  0.5703,  -1.8387,  -1.8647,   0.5250,   2.8710,  -3.3747,\n",
      "          -2.2278,  -0.4556,   4.7684,   3.4968,  -1.7404,  -2.6727,\n",
      "           1.0885,  -4.6514,  -5.6628,  -2.3680,  -7.6605,  14.2798,\n",
      "          -2.9877],\n",
      "        [  3.6635,   0.0061,   1.0066,  -0.0280,  -4.8533,  -2.2711,\n",
      "           1.0757,  -3.0647,   0.1250,  -4.1093,   0.5271,  -0.1887,\n",
      "           4.9077,  -2.6987,  -5.1596,   1.9789,   5.7830,  -3.9066,\n",
      "          14.0224],\n",
      "        [  6.4908,   2.1317,  15.3853,  -6.2313,   5.5215,  -6.2207,\n",
      "           4.0508,  -0.7365, -10.5885,   2.4885,  -4.7149,  -6.1856,\n",
      "           0.2718,  -2.4216,   2.1166,   0.8392,  -7.7890,  -3.6800,\n",
      "          -3.1491]])\n",
      "torch.Size([41, 19])\n",
      "idx: tensor([  7,   0,  12,  12,   3,  15,  13,  10,   9,   3,   1,   8,\n",
      "          7,   2,  15,  12,   0,   9,  11,   4,  15,   4,   9,   5,\n",
      "          6,  10,  15,   6,   0,   0,  13,  15,  12,   0,  14,   1,\n",
      "         17,  16,  17,  18,   2])\n",
      "idx.data.numpy(): [ 7  0 12 12  3 15 13 10  9  3  1  8  7  2 15 12  0  9 11  4 15  4  9  5\n",
      "  6 10 15  6  0  0 13 15 12  0 14  1 17 16 17 18  2]\n",
      "idx.squeeze(): [ 7  0 12 12  3 15 13 10  9  3  1  8  7  2 15 12  0  9 11  4 15  4  9  5\n",
      "  6 10 15  6  0  0 13 15 12  0 14  1 17 16 17 18  2]\n"
     ]
    }
   ],
   "source": [
    "# 학습된 모델로 test\n",
    "outputs = model(inputs) # forwarding\n",
    "print(\"outputs:\", outputs)\n",
    "print(outputs.shape)\n",
    "_, idx = outputs.max(1) \n",
    "\"\"\"\n",
    "outputs.max(axis=1)의 의미: \n",
    "outputs의 shape는 [41, 19] => 총 41의 길이를 가지는 문자열. 각 문자는 19가지의 경우의 수(class=19개)\n",
    "max(axis=1)의 의미는 각 덩어리(41개의 덩어리) 안에서 가장 큰 값을 가지는 한 원소를 골라내는 것.\n",
    "즉, 41개의 문자열 자리에서 각 자리에 올 수 있는 19개의 경우의 수 중 가장 큰 확률을 가지는 문자를 선택.\n",
    "결과값는 41의 길이를 가지는 문자열이 된다.\n",
    "\"\"\"\n",
    "print(\"idx:\", idx)\n",
    "idx = idx.data.numpy()\n",
    "print(\"idx.data.numpy():\", idx)\n",
    "print(\"idx.squeeze():\", idx.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', 'e', 'l', 'l', 'o', ' ', 'p', 'y', 't', 'o', 'r', 'c', 'h', '!', ' ', 'l', 'e', 't', \"'\", 's', ' ', 's', 't', 'u', 'd', 'y', ' ', 'd', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', '!']\n",
      "Predicted string: hello pytorch! let's study deep learning!\n"
     ]
    }
   ],
   "source": [
    "#결과 출력\n",
    "result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "print(result_str)\n",
    "print(\"Predicted string:\", ''.join(result_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
